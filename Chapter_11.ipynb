{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Handling imbalanced data"
      ],
      "metadata": {
        "id": "vxrpnV8aA2yW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will look at various methods for handling an imbalanced classes problem and apply Conformal Prediction to calibrate class probabilities.\n",
        "\n",
        "We will use Credit Card Fraud Detection dataset from Kaggle https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
        "\n",
        "The datasets contains credit card transactions in September 2013 by cardholders in Europe. The transactions occurred on two days, with 492 fraudulent transactions out of 284,807 transactions. The dataset is highly imbalanced, with positive class (fraudulent transactions) accounting for 0.17% of all transactions.\n",
        "\n",
        "The dataset contains numerical features that are the results of PCA transformation, the original features have been withheld due to confidentiality and privacy issues.\n",
        "\n",
        "* Features V1, V2, ... V28 are the principal components obtained using PCA;\n",
        "* The only original features are 'Time' and 'Amount'.\n",
        "* Feature 'Time' contains the time (in seconds) for each transaction relative to the first transaction in the dataset.\n",
        "* The feature 'Amount' is the transaction Amount.\n",
        "* Label 'Class' is the dependant variable that needs to be predicted (fraudulent transactions labeled with 1)."
      ],
      "metadata": {
        "id": "qad9G3m4KqIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dtype_diet\n",
        "!pip install catboost\n",
        "!pip install plotly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j691gPwiJ_BZ",
        "outputId": "a9d40423-7df9-41d6-d7fa-2482466dbafb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dtype_diet\n",
            "  Downloading dtype_diet-0.0.2-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from dtype_diet) (1.5.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from dtype_diet) (0.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->dtype_diet) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->dtype_diet) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->dtype_diet) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.0->dtype_diet) (1.16.0)\n",
            "Installing collected packages: dtype_diet\n",
            "Successfully installed dtype_diet-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import time\n",
        "\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = 'colab'\n",
        "\n",
        "# Set the style for visualization\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from dtype_diet import report_on_dataframe, optimize_dtypes\n",
        "\n",
        "import plotly.graph_objs as go\n",
        "import plotly.figure_factory as ff\n",
        "from plotly import tools\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "from plotly.subplots import make_subplots\n",
        "init_notebook_mode(connected=True)\n",
        "\n",
        "import gc\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, average_precision_score, cohen_kappa_score, recall_score, f1_score, roc_auc_score, log_loss, brier_score_loss, matthews_corrcoef\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.calibration import CalibrationDisplay, calibration_curve\n",
        "from matplotlib.gridspec import GridSpec\n",
        "\n",
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler, NearMiss, TomekLinks, EditedNearestNeighbours\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn import svm\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "\n",
        "pd.set_option('display.max_columns', 100)"
      ],
      "metadata": {
        "id": "Bqdo4hGIBOq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "upYaj3MNJQK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/creditcard/creditcard.csv')"
      ],
      "metadata": {
        "id": "nO0pWBcLFGKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Original df memory: {data.memory_usage(deep=True).sum()/1024/1024} MB')\n",
        "proposed_df = report_on_dataframe(data, unit=\"MB\")\n",
        "\n",
        "data = optimize_dtypes(data, proposed_df)\n",
        "print(f'Proposed df memory: {data.memory_usage(deep=True).sum()/1024/1024} MB')"
      ],
      "metadata": {
        "id": "apzB2ianKIUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "eKRgeFhWJgFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "Y6cFbmVHJ1lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# there is no missing data in the dataset\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "5uKxPI3pTTHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA"
      ],
      "metadata": {
        "id": "hP76XkOkRDmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Examination of the Data:\n",
        "\n",
        "The dataset contains 284,807 records and 31 columns.\n",
        "The columns consist of:\n",
        "* Time: Number of seconds elapsed between this transaction and the first transaction in the dataset.\n",
        "* V1 to V28: These are the principal components obtained through PCA.\n",
        "Amount: Transaction amount.\n",
        "* Class: This is our target variable where 1 indicates a fraudulent transaction and 0 indicates a non-fraudulent transaction.\n",
        "\n",
        "Summary Statistics Insights:\n",
        "\n",
        "Time: Ranges from 0 to 172,792 seconds. This indicates that the data spans over roughly two days of transactions.\n",
        "* Amount: The average transaction amount is about 88.35, with a standard deviation of 250.12.\n",
        "* Transaction amounts range from 0 to 25,691.16.\n",
        "* Class: The mean value is close to 0 (0.001727 to be exact), which indicates a highly imbalanced dataset, as expected."
      ],
      "metadata": {
        "id": "nhYZIFV1RGW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "e0nMu1rhLdtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the visualizations from the Exploratory Data Analysis (EDA):\n",
        "\n",
        "Distribution of Fraudulent vs Non-Fraudulent Transactions:\n",
        "\n",
        "As expected, the dataset is highly imbalanced with a vast majority of transactions being non-fraudulent.\n",
        "Distribution of Transaction Times:\n",
        "\n",
        "The distribution seems bimodal, suggesting two peaks or high activity periods within the 2-day span of transactions. This could possibly correspond to daytime activities and nighttime.\n",
        "Distribution of Transaction Amount:\n",
        "\n",
        "Most of the transaction amounts are concentrated around the lower values, with very few high-value transactions.\n",
        "Distribution of V1 for Fraudulent vs Non-Fraudulent Transactions:\n",
        "\n",
        "This distribution plot for one of the PCA components (V1) shows that the feature distributions for fraudulent and non-fraudulent transactions have some differences, which could be useful for classification."
      ],
      "metadata": {
        "id": "uItwLpbASwWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the figure and axes\n",
        "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(18, 12))\n",
        "\n",
        "# Distribution of the target variable (Class)\n",
        "sns.countplot(data=data, x='Class', ax=ax[0, 0])\n",
        "ax[0, 0].set_title('Distribution of Fraudulent vs Non-Fraudulent Transactions')\n",
        "ax[0, 0].set_xticklabels(['Non-Fraudulent (0)', 'Fraudulent (1)'])\n",
        "\n",
        "# Distribution of the Time column\n",
        "sns.histplot(data['Time'], ax=ax[0, 1], bins=50)\n",
        "ax[0, 1].set_title('Distribution of Transaction Times')\n",
        "\n",
        "# Distribution of the Amount column\n",
        "sns.histplot(data['Amount'], ax=ax[1, 0], bins=100)\n",
        "ax[1, 0].set_title('Distribution of Transaction Amount')\n",
        "ax[1, 0].set_xlim([0, 2000])  # Limiting for better visualization as there are few high value transactions\n",
        "\n",
        "# Distribution of one of the PCA components (V1 as an example)\n",
        "sns.kdeplot(data[data['Class'] == 0]['V1'], label='Non-Fraudulent', ax=ax[1, 1])\n",
        "sns.kdeplot(data[data['Class'] == 1]['V1'], label='Fraudulent', ax=ax[1, 1])\n",
        "ax[1, 1].set_title('Distribution of V1 for Fraudulent vs Non-Fraudulent Transactions')\n",
        "ax[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qlY0JbhARQKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fraudulent transactions exhibit a more uniform distribution over time compared to valid transactions. They appear consistently distributed throughout the timeline, even during periods with low genuine transaction activity, which corresponds to nighttime in the European timezone."
      ],
      "metadata": {
        "id": "lesD6OvKVdhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the figure with the specified colors\n",
        "plt.figure(figsize=(9, 6))\n",
        "\n",
        "# Plot the density distribution of 'Time' for non-fraudulent transactions using the same blue color\n",
        "sns.kdeplot(data[data['Class'] == 0]['Time'], color='blue', label='Non-Fraudulent', fill=True, alpha=0.5)\n",
        "\n",
        "# Overlay the density distribution of 'Time' for fraudulent transactions using the same red color\n",
        "sns.kdeplot(data[data['Class'] == 1]['Time'], color='red', label='Fraudulent', fill=True, alpha=0.5)\n",
        "\n",
        "plt.title('Density Distribution of Transaction Time for Both Classes')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p6cQGzl0Ry0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Time' from seconds to hours, without limiting to a single day's range\n",
        "data['Hour'] = (data['Time'] / 3600).astype(int)\n",
        "\n",
        "# Grouping data by 'Hour' and 'Class' to get count and sum of transactions for each class\n",
        "grouped_data = data.groupby(['Hour', 'Class']).agg(Number_of_Transactions=('Time', 'count'), Total_Amount=('Amount', 'sum')).reset_index()\n",
        "\n",
        "grouped_data.head()"
      ],
      "metadata": {
        "id": "4pylqh2JUmE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot normalized hourly distributions, such that, for each class, the values represent a percentage of the daily total for that class.\n",
        "# Calculate daily totals for number of transactions and total amount for each class\n",
        "daily_totals_by_class = grouped_data.groupby(['Class']).agg(Daily_Total_Transactions=('Number_of_Transactions', 'sum'),\n",
        "                                                            Daily_Total_Amount=('Total_Amount', 'sum')).reset_index()\n",
        "\n",
        "# Merge these daily totals with the original grouped data\n",
        "normalized_data = pd.merge(grouped_data, daily_totals_by_class, on='Class', how='left')\n",
        "\n",
        "# Calculate the percentage of transactions and amounts for each hour based on the daily totals for each class\n",
        "normalized_data['Percentage_Transactions'] = (normalized_data['Number_of_Transactions'] / normalized_data['Daily_Total_Transactions']) * 100\n",
        "normalized_data['Percentage_Amount'] = (normalized_data['Total_Amount'] / normalized_data['Daily_Total_Amount']) * 100\n",
        "\n",
        "normalized_data.head()"
      ],
      "metadata": {
        "id": "xv_DwU1xdy7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the corrected hourly percentages\n",
        "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12, 10))\n",
        "\n",
        "# Hourly distribution of the percentage of transactions for both classes\n",
        "sns.lineplot(data=normalized_data, x='Hour', y='Percentage_Transactions', hue='Class', ax=ax[0], palette='tab10')\n",
        "ax[0].set_title('Hourly Distribution of Percentage of Transactions')\n",
        "ax[0].set_ylabel('Percentage of Transactions (%)')\n",
        "ax[0].legend(title='Class', labels=['Non-Fraudulent', 'Fraudulent'])\n",
        "\n",
        "# Hourly distribution of the percentage of transaction amounts for both classes\n",
        "sns.lineplot(data=normalized_data, x='Hour', y='Percentage_Amount', hue='Class', ax=ax[1], palette='tab10')\n",
        "ax[1].set_title('Hourly Distribution of Percentage of Transaction Amounts')\n",
        "ax[1].set_ylabel('Percentage of Transaction Amounts (%)')\n",
        "ax[1].legend(title='Class', labels=['Non-Fraudulent', 'Fraudulent'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fxzdukhSbRxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import plotly.io as pio\n",
        "pio.renderers.default = 'colab'"
      ],
      "metadata": {
        "id": "eKhcnxlKrtDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Plotly line chart for hourly distribution of the percentage of transactions for both classes\n",
        "fig1 = go.Figure()\n",
        "\n",
        "# Add traces for non-fraudulent and fraudulent transactions\n",
        "fig1.add_trace(go.Scatter(x=normalized_data[normalized_data['Class'] == 0]['Hour'],\n",
        "                          y=normalized_data[normalized_data['Class'] == 0]['Percentage_Transactions'],\n",
        "                          mode='lines',\n",
        "                          name='Non-Fraudulent'))\n",
        "fig1.add_trace(go.Scatter(x=normalized_data[normalized_data['Class'] == 1]['Hour'],\n",
        "                          y=normalized_data[normalized_data['Class'] == 1]['Percentage_Transactions'],\n",
        "                          mode='lines',\n",
        "                          name='Fraudulent'))\n",
        "\n",
        "# Update layout\n",
        "fig1.update_layout(title='Hourly Distribution of Percentage of Transactions',\n",
        "                   xaxis_title='Hour',\n",
        "                   yaxis_title='Percentage of Transactions (%)')\n",
        "\n",
        "# Create a Plotly line chart for hourly distribution of the percentage of transaction amounts for both classes\n",
        "fig2 = go.Figure()\n",
        "\n",
        "# Add traces for non-fraudulent and fraudulent transactions\n",
        "fig2.add_trace(go.Scatter(x=normalized_data[normalized_data['Class'] == 0]['Hour'],\n",
        "                          y=normalized_data[normalized_data['Class'] == 0]['Percentage_Amount'],\n",
        "                          mode='lines',\n",
        "                          name='Non-Fraudulent'))\n",
        "fig2.add_trace(go.Scatter(x=normalized_data[normalized_data['Class'] == 1]['Hour'],\n",
        "                          y=normalized_data[normalized_data['Class'] == 1]['Percentage_Amount'],\n",
        "                          mode='lines',\n",
        "                          name='Fraudulent'))\n",
        "\n",
        "# Update layout\n",
        "fig2.update_layout(title='Hourly Distribution of Percentage of Transaction Amounts',\n",
        "                   xaxis_title='Hour',\n",
        "                   yaxis_title='Percentage of Transaction Amounts (%)')\n",
        "\n",
        "fig1.show()\n",
        "fig2.show()"
      ],
      "metadata": {
        "id": "qvQQoZ3WeCY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Plotly boxplot for transaction amounts for both classes\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add boxplots for non-fraudulent and fraudulent transaction amounts\n",
        "fig.add_trace(go.Box(y=data[data['Class'] == 0]['Amount'], name='Non-Fraudulent', marker_color='blue'))\n",
        "fig.add_trace(go.Box(y=data[data['Class'] == 1]['Amount'], name='Fraudulent', marker_color='red'))\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(title='Boxplot of Transaction Amounts',\n",
        "                  yaxis=dict(type='log', title='Transaction Amount ($)'),\n",
        "                  xaxis_title='Class')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "gZkoKrePfhBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out fraudulent transactions\n",
        "fraudulent_data = data[data['Class'] == 1]\n",
        "\n",
        "# Create a histogram with binned time intervals\n",
        "hist_data, bin_edges = np.histogram(fraudulent_data['Time'], bins=48)  # 48 bins for 48 hours"
      ],
      "metadata": {
        "id": "X9BNi506hBAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a heatmap using Plotly's graph_objects\n",
        "fig = go.Figure(data=go.Heatmap(z=[hist_data], x=bin_edges[:-1], colorscale='Viridis', showscale=True))\n",
        "\n",
        "# Update layout and axis titles\n",
        "fig.update_layout(title='Heatmap of Fraudulent Transactions Over Time',\n",
        "                  xaxis_title='Time (in seconds)',\n",
        "                  yaxis_title='Fraudulent Transactions',\n",
        "                  yaxis_nticks=1)  # Only one y-tick as we have one row of data\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "fig.update_xaxes(tickangle=45)\n",
        "\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "imCCFTazghdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix\n",
        "correlation_matrix = data.corr()\n",
        "\n",
        "# Create a heatmap using Plotly\n",
        "heatmap = go.Figure(data=go.Heatmap(z=correlation_matrix.values,\n",
        "                                    x=correlation_matrix.columns,\n",
        "                                    y=correlation_matrix.columns,\n",
        "                                    colorscale='Viridis',\n",
        "                                    zmin=-1, zmax=1))\n",
        "\n",
        "# Update layout for better readability\n",
        "heatmap.update_layout(title='Correlation Heatmap of Features and Target',\n",
        "                      xaxis_tickangle=-45)\n",
        "\n",
        "heatmap.show()\n"
      ],
      "metadata": {
        "id": "yGiZ5tKxg1-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the correlation matrix again\n",
        "correlation_matrix = data.corr()\n",
        "\n",
        "# Display the correlation matrix using a red to green heatmap formatting on the dataframe display\n",
        "cm_red_green = sns.diverging_palette(150, 10, as_cmap=True)\n",
        "styled_correlation_red_green = correlation_matrix.style.background_gradient(cmap=cm_red_green)\n",
        "styled_correlation_red_green"
      ],
      "metadata": {
        "id": "IifmfQuRsDFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recompute the correlations of features with 'Class'\n",
        "class_correlations = correlation_matrix[\"Class\"].drop(\"Class\")\n",
        "\n",
        "# Create the figure\n",
        "fig = go.Figure(data=[go.Bar(x=class_correlations.index,\n",
        "                             y=class_correlations.values,\n",
        "                             marker=dict(color=class_correlations.values,\n",
        "                                         colorscale=\"RdYlGn\",\n",
        "                                         colorbar=dict(title=\"Correlation Coefficient\")))])\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(title=\"Correlation of Features with 'Class'\",\n",
        "                  xaxis_title=\"Features\",\n",
        "                  yaxis_title=\"Correlation Coefficient\",\n",
        "                  xaxis_tickangle=-45)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "K-MrgAxTui4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling"
      ],
      "metadata": {
        "id": "39Fq5ZJCy_h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TRAIN/VALIDATION/TEST SPLIT\n",
        "VALID_SIZE = 0.20 # simple validation using train_test_split\n",
        "TEST_SIZE = 0.20 # test size using_train_test_split\n",
        "\n",
        "#CROSS-VALIDATION\n",
        "NUMBER_KFOLDS = 5 #number of KFolds for cross-validation\n",
        "\n",
        "RANDOM_STATE = 42"
      ],
      "metadata": {
        "id": "1RPG6tR4z9S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = 'Class'\n",
        "features = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10','V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n",
        "       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28','Amount']"
      ],
      "metadata": {
        "id": "MDZLyIFdzsTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train, validation, test, and calibration sets\n",
        "train_calib_df, test_df = train_test_split(data, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=False)\n",
        "train_df, calib_df = train_test_split(train_calib_df, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=False)\n",
        "len(train_df), len(calib_df), len(test_df)  # Display the number of samples in each dataset\n"
      ],
      "metadata": {
        "id": "YbNvM_w6z1ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "BslSqIvI0usB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop('Hour',axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "jJt5aMSQ0uuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "lSFGEMUp0uy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the StandardScaler on the training set\n",
        "scaler_time = StandardScaler().fit(train_df['Time'].values.reshape(-1, 1))\n",
        "scaler_amount = StandardScaler().fit(train_df['Amount'].values.reshape(-1, 1))\n",
        "\n",
        "# Transform the 'Time' and 'Amount' columns in train, validation, calibration, and test sets\n",
        "train_df['Time'] = scaler_time.transform(train_df['Time'].values.reshape(-1, 1))\n",
        "train_df['Amount'] = scaler_amount.transform(train_df['Amount'].values.reshape(-1, 1))\n",
        "\n",
        "calib_df['Time'] = scaler_time.transform(calib_df['Time'].values.reshape(-1, 1))\n",
        "calib_df['Amount'] = scaler_amount.transform(calib_df['Amount'].values.reshape(-1, 1))\n",
        "\n",
        "test_df['Time'] = scaler_time.transform(test_df['Time'].values.reshape(-1, 1))\n",
        "test_df['Amount'] = scaler_amount.transform(test_df['Amount'].values.reshape(-1, 1))\n",
        "\n",
        "train_df[['Time', 'Amount']].head()  # Display the transformed 'Time' and 'Amount' columns for the training set as an example\n"
      ],
      "metadata": {
        "id": "e9VDsL7v0u1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for overlapping rows between the training and test sets\n",
        "overlapping_rows = train_df.merge(test_df, how='inner')\n",
        "overlap_count = overlapping_rows.shape[0]\n",
        "\n",
        "overlap = overlap_count > 0\n",
        "overlap, overlap_count"
      ],
      "metadata": {
        "id": "CSh-Wre3FZkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "pfxsbPsw2HWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a data frame# Creating a dataframe to record the # Creating a dataframe to record performance of various models\n",
        "models = ['Naive Classifier', 'Logistic Regression', 'Random Forest Classifier', 'AdaBoost Classifier', 'CatBoost Classifier', 'SVC', 'LGBM Classifier', 'XGBoost Classifier']\n",
        "\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC', 'ECE', 'Log Loss', 'Brier Loss']\n",
        "\n",
        "performance_base_models_df = pd.DataFrame(index=models, columns=metrics)\n",
        "\n",
        "performance_calibrated_models_df = pd.DataFrame(index=models, columns=metrics)\n"
      ],
      "metadata": {
        "id": "RIMIA5pW3Izu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to record execution times\n",
        "time_df = pd.DataFrame(index=models, columns=['Execution Time (s)'])"
      ],
      "metadata": {
        "id": "_WFNYTq7PTAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive classifier"
      ],
      "metadata": {
        "id": "hzBMLPFk1SKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive classifier: predict the majority class (Class 0) for all samples\n",
        "naive_predictions = np.zeros(len(test_df))\n",
        "\n",
        "# True labels for the test set\n",
        "true_labels = test_df['Class'].values\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(true_labels, naive_predictions)\n",
        "precision = precision_score(true_labels, naive_predictions,zero_division=0)\n",
        "recall = recall_score(true_labels, naive_predictions,zero_division=0)\n",
        "f1 = f1_score(true_labels, naive_predictions)\n",
        "roc_auc = roc_auc_score(true_labels, naive_predictions)\n",
        "logloss = log_loss(true_labels, naive_predictions)\n",
        "brier_loss = brier_score_loss(true_labels, naive_predictions)\n",
        "\n",
        "# Compute Expected Calibration Error (ECE)\n",
        "fraction_of_positives, mean_predicted_value = calibration_curve(true_labels, naive_predictions, n_bins=10)\n",
        "ece = np.sum(np.abs(fraction_of_positives - mean_predicted_value)) / len(mean_predicted_value)\n",
        "\n",
        "# Populate the performance dataframe\n",
        "performance_base_models_df.loc['Naive Classifier', :] = [accuracy, precision, recall, f1, roc_auc, ece, logloss, brier_loss]\n"
      ],
      "metadata": {
        "id": "22-KjOVE2Fp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_base_models_df"
      ],
      "metadata": {
        "id": "_n9gKFQm7dH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_performance(model, model_name, true_labels=true_labels, performance_df=performance_base_models_df, verbose=False):\n",
        "    \"\"\"\n",
        "    Evaluates model performance and updates the performance dataframe with metrics.\n",
        "\n",
        "    Args:\n",
        "    - predictions (array-like): Predicted values from the model.\n",
        "    - model_name (str): Name of the model for which performance is being evaluated.\n",
        "    - true_labels (array-like): Actual labels for comparison. Default is the true_labels of the test set.\n",
        "    - performance_df (DataFrame): DataFrame to update with model performance metrics.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with updated performance metrics for the given model.\n",
        "    \"\"\"\n",
        "\n",
        "    start_time = time.time()  # Record start time\n",
        "\n",
        "    # Train Logistic Regression\n",
        "    model.fit(train_df.drop(columns='Class'), train_df['Class'])\n",
        "\n",
        "    # Predict class score on the test set\n",
        "    prob_pos = model.predict_proba(test_df.drop(columns='Class'))[:, 1]\n",
        "    # Predict on the test set\n",
        "    predictions = model.predict(test_df.drop(columns='Class'))\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
        "    recall = recall_score(true_labels, predictions)\n",
        "    f1 = f1_score(true_labels, predictions)\n",
        "    roc_auc = roc_auc_score(true_labels, prob_pos)\n",
        "    logloss = log_loss(true_labels, prob_pos)\n",
        "    brier_loss = brier_score_loss(true_labels, prob_pos)\n",
        "\n",
        "    # Compute Expected Calibration Error (ECE)\n",
        "    fraction_of_positives, mean_predicted_value = calibration_curve(true_labels, predictions, n_bins=10)\n",
        "    ece = np.sum(np.abs(fraction_of_positives - mean_predicted_value)) / len(mean_predicted_value)\n",
        "\n",
        "    # Populate the performance dataframe\n",
        "    performance_df.loc[model_name, :] = [accuracy, precision, recall, f1, roc_auc, ece, logloss, brier_loss]\n",
        "\n",
        "    # Plot calibration curve and histogram if verbose is True\n",
        "    if verbose:\n",
        "        fig = plt.figure(figsize=(10, 10))\n",
        "        gs = GridSpec(2, 1)\n",
        "        ax_calibration_curve = fig.add_subplot(gs[0, :])\n",
        "        ax_histogram = fig.add_subplot(gs[1, :])\n",
        "\n",
        "        # Plot calibration curve\n",
        "        CalibrationDisplay.from_estimator(\n",
        "            model,\n",
        "            test_df.drop(columns='Class'),\n",
        "            true_labels,\n",
        "            n_bins=10,\n",
        "            name=model_name,\n",
        "            ax=ax_calibration_curve\n",
        "        )\n",
        "        ax_calibration_curve.set_title(f\"Calibration plot ({model_name})\")\n",
        "\n",
        "        # Plot histogram\n",
        "        ax_histogram.hist(prob_pos, range=(0, 1), bins=10, label=model_name)\n",
        "        ax_histogram.set(title=model_name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    end_time = time.time()  # Record end time\n",
        "    execution_time = end_time - start_time  # Calculate execution time in seconds\n",
        "\n",
        "    # Record execution time in the time DataFrame\n",
        "    time_df.loc[model_name, 'Execution Time (s)'] = execution_time\n",
        "\n",
        "    return performance_df,model"
      ],
      "metadata": {
        "id": "GkP3ZJME76qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_df"
      ],
      "metadata": {
        "id": "OIXQVYm7HlZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dummy Classifier"
      ],
      "metadata": {
        "id": "F0qPB47Iaz5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_classifier_model = DummyClassifier(strategy='most_frequent', random_state=RANDOM_STATE , constant=None)\n",
        "\n",
        "performance_base_models_df, _ = evaluate_model_performance(dummy_classifier_model,'Naive Classifier', verbose=True)\n"
      ],
      "metadata": {
        "id": "Qvic65AmawKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_base_models_df"
      ],
      "metadata": {
        "id": "SmbOj4k9bhaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_df"
      ],
      "metadata": {
        "id": "4b5WrRB3blYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "ekZ1xwF88YY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Logistic Regression\n",
        "logistic_regression_model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
        "performance_base_models_df,_ = evaluate_model_performance(logistic_regression_model,'Logistic Regression', verbose=True)"
      ],
      "metadata": {
        "id": "amJhsqmH8EF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_base_models_df"
      ],
      "metadata": {
        "id": "5PoR6B6IH5XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_df"
      ],
      "metadata": {
        "id": "OWAQfN3QH6BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "uRDuQB21zOSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
        "performance_base_models_df, rf_trained_model = evaluate_model_performance(rf_model,'Random Forest Classifier', verbose=True)"
      ],
      "metadata": {
        "id": "IsZTJVho0tjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_base_models_df"
      ],
      "metadata": {
        "id": "EH0_9wOSBmUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_df"
      ],
      "metadata": {
        "id": "SMZdKdKWILU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pio.renderers.default = 'colab'\n",
        "# Extract feature importances from the Random Forest model\n",
        "feature_importances = rf_trained_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for the feature importances\n",
        "features_df = pd.DataFrame({\n",
        "    'Feature': train_df.drop(columns='Class').columns,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by importance\n",
        "features_df = features_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plotting using Plotly\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(x=features_df['Feature'], y=features_df['Importance'], marker_color='rgba(55, 128, 191, 0.7)')\n",
        "])\n",
        "\n",
        "fig.update_layout(title='Feature Importances from Random Forest',\n",
        "                  xaxis_title='Features',\n",
        "                  yaxis_title='Importance',\n",
        "                  xaxis_tickangle=-45)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "vW8XgbJkKw5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Â AdaBoost Classifier"
      ],
      "metadata": {
        "id": "7vNWxQWgLHYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train AdaBoost classifier\n",
        "ada_model = AdaBoostClassifier(random_state=RANDOM_STATE)\n",
        "performance_base_models_df, ada_trained_model = evaluate_model_performance(ada_model ,'AdaBoost Classifier', verbose=True)"
      ],
      "metadata": {
        "id": "PT74zxmkKw07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_base_models_df"
      ],
      "metadata": {
        "id": "iXW986bPNA1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_df"
      ],
      "metadata": {
        "id": "FPcaS6jZdHmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pio.renderers.default = 'colab'\n",
        "# Extract feature importances from the Random Forest model\n",
        "feature_importances = ada_trained_model.feature_importances_\n",
        "\n",
        "# Create a DataFrame for the feature importances\n",
        "features_df = pd.DataFrame({\n",
        "    'Feature': train_df.drop(columns='Class').columns,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by importance\n",
        "features_df = features_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plotting using Plotly\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(x=features_df['Feature'], y=features_df['Importance'], marker_color='rgba(55, 128, 191, 0.7)')\n",
        "])\n",
        "\n",
        "fig.update_layout(title='Feature Importances from Ada Boost',\n",
        "                  xaxis_title='Features',\n",
        "                  yaxis_title='Importance',\n",
        "                  xaxis_tickangle=-45)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "7q4QfybMKwu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CatBoost Classifier"
      ],
      "metadata": {
        "id": "huN9urLTMp3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train CatBoost classifier\n",
        "catboost_model  = CatBoostClassifier(\n",
        "    task_type=\"CPU\",       # You can change this to \"GPU\" if you have a GPU.\n",
        "    thread_count=-1,       # Use all available CPU cores\n",
        "    verbose=0,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "performance_base_models_df, catboost_trained_model = evaluate_model_performance(catboost_model,'CatBoost Classifier', verbose=True)"
      ],
      "metadata": {
        "id": "_AVng7tpKwnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_base_models_df"
      ],
      "metadata": {
        "id": "I__9hBIjePp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_df"
      ],
      "metadata": {
        "id": "ulqAWrq9eQgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pio.renderers.default = 'colab'\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importances = catboost_trained_model.get_feature_importance()\n",
        "\n",
        "# Sort feature importances\n",
        "sorted_indices = feature_importances.argsort()[::-1]  # Sort in descending order\n",
        "\n",
        "# Sort feature names based on importance order\n",
        "sorted_features = train_df.drop(columns='Class').columns[sorted_indices]\n",
        "sorted_importances = feature_importances[sorted_indices]\n",
        "\n",
        "# Plot sorted feature importances using Plotly\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(x=sorted_features,\n",
        "           y=sorted_importances,\n",
        "           marker_color='indianred')\n",
        "])\n",
        "\n",
        "fig.update_layout(title='Feature Importances from CatBoost Classifier (Sorted)',\n",
        "                 xaxis=dict(title='Features'),\n",
        "                 yaxis=dict(title='Importance'),\n",
        "                 xaxis_tickangle=-45)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TerIbEGsKwc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support Vector Machines Classifier"
      ],
      "metadata": {
        "id": "Hrp_DTzyQkLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train SVC model\n",
        "svc_model = SVC(probability=True, random_state=RANDOM_STATE)\n",
        "performance_base_models_df,_ = evaluate_model_performance(svc_model ,'SVC', verbose=True)"
      ],
      "metadata": {
        "id": "SOtQP1LVKvlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_base_models_df"
      ],
      "metadata": {
        "id": "ZWsakAC9eHd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_df"
      ],
      "metadata": {
        "id": "Y3p4gcgleKCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LGBM Classifier"
      ],
      "metadata": {
        "id": "Fa-1z13oRSKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lgb_model = lgb.LGBMClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
        "performance_base_models_df,lgb_trained_model = evaluate_model_performance(lgb_model,'LGBM Classifier', verbose=True)"
      ],
      "metadata": {
        "id": "kzDOdG-keJYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_base_models_df"
      ],
      "metadata": {
        "id": "x-lbMnxWflkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_df"
      ],
      "metadata": {
        "id": "5X8dsbI2fmEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pio.renderers.default = 'colab'\n",
        "# Extract feature importances from the Random Forest model\n",
        "feature_importances = lgb_trained_model.feature_importances_\n",
        "# Create a DataFrame for the feature importances\n",
        "features_df = pd.DataFrame({\n",
        "    'Feature': train_df.drop(columns='Class').columns,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by importance\n",
        "features_df = features_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plotting using Plotly\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(x=features_df['Feature'], y=features_df['Importance'], marker_color='rgba(55, 128, 191, 0.7)')\n",
        "])\n",
        "\n",
        "fig.update_layout(title='Feature Importances from Ada Boost',\n",
        "                  xaxis_title='Features',\n",
        "                  yaxis_title='Importance',\n",
        "                  xaxis_tickangle=-45)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "hcbO1kgaRG0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost classifier"
      ],
      "metadata": {
        "id": "c4RwSFpMX-X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Train the XGBoost Classifier\n",
        "xgb_model = xgb.XGBClassifier(n_jobs=-1)\n",
        "performance_base_models_df,xgb_trained_model = evaluate_model_performance(xgb_model,'XGBoost Classifier', verbose=True)"
      ],
      "metadata": {
        "id": "hqGcAQuhRGJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pio.renderers.default = 'colab'\n",
        "# Extract feature importances from the Random Forest model\n",
        "feature_importances = xgb_trained_model.feature_importances_\n",
        "# Create a DataFrame for the feature importances\n",
        "features_df = pd.DataFrame({\n",
        "    'Feature': train_df.drop(columns='Class').columns,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by importance\n",
        "features_df = features_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plotting using Plotly\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(x=features_df['Feature'], y=features_df['Importance'], marker_color='rgba(55, 128, 191, 0.7)')\n",
        "])\n",
        "\n",
        "fig.update_layout(title='Feature Importances from Ada Boost',\n",
        "                  xaxis_title='Features',\n",
        "                  yaxis_title='Importance',\n",
        "                  xaxis_tickangle=-45)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "7LA1nso3gQlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_base_models_df"
      ],
      "metadata": {
        "id": "cp-eFwpKgKz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_df"
      ],
      "metadata": {
        "id": "KUU7QEVNgLJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_base_models_df.to_csv('performance_base_models.csv')"
      ],
      "metadata": {
        "id": "r7Q_vPDKn28Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_base_models_df.sort_values(['Log Loss',\t'Brier Loss'], ascending = [True, True])"
      ],
      "metadata": {
        "id": "_Dq1VHAagzwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_base_models_df.sort_values(['ECE'], ascending = True)"
      ],
      "metadata": {
        "id": "f0F7WXD5g7Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_base_models_df.sort_values(['Log Loss', \"Brier Loss\"])"
      ],
      "metadata": {
        "id": "xLuPj63Z-AXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_df.columns"
      ],
      "metadata": {
        "id": "2_3PY3Y_rTVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time_df.sort_values('Execution Time (s)', ascending = True)"
      ],
      "metadata": {
        "id": "fqXoXNqo6pgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Investigate resampling techniques"
      ],
      "metadata": {
        "id": "9-NzjYvxB91W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_time_df = pd.DataFrame(index = time_df.index, columns = time_df.columns)\n",
        "resampled_time_df"
      ],
      "metadata": {
        "id": "Kj4ONpg7FZS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_resampling_methods_df = pd.DataFrame(index = metrics, columns = ['None', 'Weights', 'Threshold', 'Threshold + W', 'RandomOverSampler',\\\n",
        "                                    'SMOTE', 'ADASYN', 'RandomUnderSampler', 'NearMiss', 'TomekLinks', 'EditedNearestNeighbours'])"
      ],
      "metadata": {
        "id": "bn_H3DuzCh9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_resampled_model_performance(model, model_name, sampler, true_labels=true_labels, performance_df=performance_resampling_methods_df, verbose=False):\n",
        "    \"\"\"\n",
        "    Evaluates model performance and updates the performance dataframe with metrics.\n",
        "\n",
        "    Args:\n",
        "    - predictions (array-like): Predicted values from the model.\n",
        "    - model_name (str): Name of the model for which performance is being evaluated.\n",
        "    - true_labels (array-like): Actual labels for comparison. Default is the true_labels of the test set.\n",
        "    - performance_df (DataFrame): DataFrame to update with model performance metrics.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with updated performance metrics for the given model.\n",
        "    \"\"\"\n",
        "\n",
        "    start_time = time.time()  # Record start time\n",
        "\n",
        "    technique = sampler.__class__.__name__\n",
        "\n",
        "    X_resampled, y_resampled = sampler.fit_resample(train_df.drop(columns='Class'), train_df['Class'])\n",
        "\n",
        "    # Train Logistic Regression\n",
        "    model.fit(X_resampled, y_resampled)\n",
        "\n",
        "    # Predict class score on the test set\n",
        "    prob_pos = model.predict_proba(test_df.drop(columns='Class'))[:, 1]\n",
        "    # Predict on the test set\n",
        "    predictions = model.predict(test_df.drop(columns='Class'))\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
        "    recall = recall_score(true_labels, predictions)\n",
        "    f1 = f1_score(true_labels, predictions)\n",
        "    roc_auc = roc_auc_score(true_labels, prob_pos)\n",
        "    logloss = log_loss(true_labels, prob_pos)\n",
        "    brier_loss = brier_score_loss(true_labels, prob_pos)\n",
        "\n",
        "    # Compute Expected Calibration Error (ECE)\n",
        "    fraction_of_positives, mean_predicted_value = calibration_curve(true_labels, prob_pos, n_bins=10)\n",
        "    ece = np.sum(np.abs(fraction_of_positives - mean_predicted_value)) / len(mean_predicted_value)\n",
        "\n",
        "    # Populate the performance dataframe\n",
        "    performance_resampling_methods_df[technique] = [accuracy, precision, recall, f1, roc_auc, ece, logloss, brier_loss]\n",
        "\n",
        "    # Plot calibration curve and histogram if verbose is True\n",
        "    if verbose:\n",
        "        fig = plt.figure(figsize=(10, 10))\n",
        "        gs = GridSpec(2, 1)\n",
        "        ax_calibration_curve = fig.add_subplot(gs[0, :])\n",
        "        ax_histogram = fig.add_subplot(gs[1, :])\n",
        "\n",
        "        # Plot calibration curve\n",
        "        CalibrationDisplay.from_estimator(\n",
        "            model,\n",
        "            test_df.drop(columns='Class'),\n",
        "            true_labels,\n",
        "            n_bins=10,\n",
        "            name=model_name,\n",
        "            ax=ax_calibration_curve\n",
        "        )\n",
        "        ax_calibration_curve.set_title(f\"Calibration plot ({model_name})\")\n",
        "\n",
        "        # Plot histogram\n",
        "        ax_histogram.hist(prob_pos, range=(0, 1), bins=10, label=model_name)\n",
        "        ax_histogram.set(title=model_name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    end_time = time.time()  # Record end time\n",
        "    execution_time = end_time - start_time  # Calculate execution time in seconds\n",
        "\n",
        "    # Record execution time in the time DataFrame\n",
        "    resampled_time_df.loc[model_name, 'Execution Time (s)'] = execution_time\n",
        "\n",
        "    return performance_df,model"
      ],
      "metadata": {
        "id": "5CFC466WD9Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Logistic Regression\n",
        "logistic_regression_model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
        "performance_resampling_methods_df,_ = evaluate_resampled_model_performance(logistic_regression_model,'Logistic Regression', sampler = SMOTE(), verbose=True)"
      ],
      "metadata": {
        "id": "WJ_4-a4_Ht0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_resampling_methods_df"
      ],
      "metadata": {
        "id": "vsr5a9sJImqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic regression with resampling methods"
      ],
      "metadata": {
        "id": "JeW0cpTz_Xzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "techniques = [RandomOverSampler(), SMOTE(), ADASYN(), RandomUnderSampler(), NearMiss(version=1), TomekLinks(), EditedNearestNeighbours()]\n",
        "\n",
        "for sampler in tqdm(techniques):\n",
        "    technique = sampler.__class__.__name__\n",
        "    print(f'Technique: {technique}')\n",
        "    model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
        "    performance_resampling_methods_df,_ = evaluate_resampled_model_performance(logistic_regression_model,'Logistic Regression', sampler = sampler, verbose=True)"
      ],
      "metadata": {
        "id": "utAJpRVqM8a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_time_df"
      ],
      "metadata": {
        "id": "K68GIkcQPUFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance_resampling_methods_df"
      ],
      "metadata": {
        "id": "Ife0XtHaM8PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_performance_resampling_methods_df = performance_resampling_methods_df.copy()\n",
        "performance_resampling_methods_df[:] = np.nan"
      ],
      "metadata": {
        "id": "FXbFDa8gO_V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_performance_resampling_methods_df.to_csv('lr_performance_resampling_methods.csv')"
      ],
      "metadata": {
        "id": "HWKJ5It62YsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sampler in tqdm(techniques):\n",
        "    technique = sampler.__class__.__name__\n",
        "\n",
        "    print(f'Technique: {technique}')\n",
        "\n",
        "    catboost_model  = CatBoostClassifier(\n",
        "    task_type=\"CPU\",       # You can change this to \"GPU\" if you have a GPU.\n",
        "    thread_count=-1,       # Use all available CPU cores\n",
        "    verbose=0,\n",
        "    random_state=RANDOM_STATE\n",
        "    )\n",
        "    performance_resampling_methods_df,_ = evaluate_resampled_model_performance(catboost_model,'CatBoost Classifier', sampler = sampler, verbose=True)"
      ],
      "metadata": {
        "id": "nT8fNQfuIKao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CatBoost with resampling methods"
      ],
      "metadata": {
        "id": "wIYAX_LR_g4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "catboost_performance_resampling_methods_df = performance_resampling_methods_df.copy()\n",
        "performance_resampling_methods_df[:] = np.nan"
      ],
      "metadata": {
        "id": "aaBauTsH2hnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "catboost_performance_resampling_methods_df"
      ],
      "metadata": {
        "id": "eh5tZglZQNNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_time_df"
      ],
      "metadata": {
        "id": "_GoZ8UPzQQBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "catboost_performance_resampling_methods_df.to_csv('catboost_performance_resampling_methods.csv')"
      ],
      "metadata": {
        "id": "jIBtRJTm2mwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calibration with Venn-ABERS"
      ],
      "metadata": {
        "id": "NNfBns_yHsil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r '/content/VennABERS'"
      ],
      "metadata": {
        "id": "KJiUXcPyH_iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLONE_URL = f\"https://github.com/ptocca/VennABERS\"\n",
        "!git clone https://github.com/ptocca/VennABERS\n",
        "get_ipython().system(f\"git clone {CLONE_URL}\")\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"VennABERS\")"
      ],
      "metadata": {
        "id": "srIzFwanHvF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd VennABERS"
      ],
      "metadata": {
        "id": "xqaNrp-iH39A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import VennABERS\n",
        "??VennABERS.ScoresToMultiProbs"
      ],
      "metadata": {
        "id": "uWy8Ufv9ICiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "c8CZkFBlIFml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '../'"
      ],
      "metadata": {
        "id": "3d-36KFxIKY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calibrate Logistic Regression"
      ],
      "metadata": {
        "id": "Pd0IyRlyI04Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calibrated_performance_df = performance_resampling_methods_df.copy()\n",
        "calibrated_performance_df[:] = np.nan"
      ],
      "metadata": {
        "id": "1aTp0E_cNiTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ground truth calibration labels\n",
        "y_cal = calib_df['Class']"
      ],
      "metadata": {
        "id": "oGb4izCoKqBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_resampled_calibrated_model_performance(model, model_name, sampler, true_labels=true_labels, performance_df=calibrated_performance_df, verbose=False):\n",
        "    \"\"\"\n",
        "    Evaluates model performance and updates the performance dataframe with metrics.\n",
        "\n",
        "    Args:\n",
        "    - predictions (array-like): Predicted values from the model.\n",
        "    - model_name (str): Name of the model for which performance is being evaluated.\n",
        "    - true_labels (array-like): Actual labels for comparison. Default is the true_labels of the test set.\n",
        "    - performance_df (DataFrame): DataFrame to update with model performance metrics.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with updated performance metrics for the given model.\n",
        "    \"\"\"\n",
        "\n",
        "    start_time = time.time()  # Record start time\n",
        "\n",
        "    technique = sampler.__class__.__name__\n",
        "\n",
        "    X_resampled, y_resampled = sampler.fit_resample(train_df.drop(columns='Class'), train_df['Class'])\n",
        "\n",
        "    # Train Logistic Regression\n",
        "    model.fit(X_resampled, y_resampled)\n",
        "\n",
        "    # use trained machnine learning model to predict on the calibration set\n",
        "    y_hat_cal_scores = model.predict_proba(calib_df.drop(columns='Class'))[:, 1]\n",
        "\n",
        "    # Predict class score on the test set\n",
        "    prob_pos = model.predict_proba(test_df.drop(columns='Class'))[:, 1]\n",
        "    testScores =prob_pos\n",
        "\n",
        "    # Predict on the test set\n",
        "    predictions = model.predict(test_df.drop(columns='Class'))\n",
        "\n",
        "    # calibrate using Venn-ABERS\n",
        "    #calibrPts: a list of pairs (score,label) corresponding to the scores and labels of the calibration examples. The score is a float and the label is an integer meant to take values 0 or 1.\n",
        "    calibrPts = zip(list(y_hat_cal_scores),list(y_cal))\n",
        "\n",
        "    # Conformal Prediciton VennABERS calibration model learns calibration\n",
        "    # on the calibration dataset by comparing scores output by underlying machine\n",
        "    # learning model and comparing them with class labels on the calibration set\n",
        "    p0,p1 = VennABERS.ScoresToMultiProbs(calibrPts,testScores)\n",
        "\n",
        "    prob_pos_calibrated = p1/(1-p0+p1)\n",
        "    calibrated_predictions = prob_pos_calibrated > 0.5\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(true_labels, calibrated_predictions)\n",
        "    precision = precision_score(true_labels, calibrated_predictions, zero_division=0)\n",
        "    recall = recall_score(true_labels, calibrated_predictions)\n",
        "    f1 = f1_score(true_labels, calibrated_predictions)\n",
        "    roc_auc = roc_auc_score(true_labels, prob_pos_calibrated)\n",
        "    logloss = log_loss(true_labels, prob_pos_calibrated)\n",
        "    brier_loss = brier_score_loss(true_labels, prob_pos_calibrated)\n",
        "\n",
        "    # Compute Expected Calibration Error (ECE)\n",
        "    fraction_of_positives, mean_predicted_value = calibration_curve(true_labels, prob_pos_calibrated, n_bins=10)\n",
        "    ece = np.sum(np.abs(fraction_of_positives - mean_predicted_value)) / len(mean_predicted_value)\n",
        "\n",
        "    # Populate the performance dataframe\n",
        "    calibrated_performance_df[technique] = [accuracy, precision, recall, f1, roc_auc, ece, logloss, brier_loss]\n",
        "\n",
        "    # Plot calibration curve and histogram if verbose is True\n",
        "    if verbose:\n",
        "        fig = plt.figure(figsize=(10, 10))\n",
        "        gs = GridSpec(2, 1)\n",
        "        ax_calibration_curve = fig.add_subplot(gs[0, :])\n",
        "        ax_histogram = fig.add_subplot(gs[1, :])\n",
        "\n",
        "        # Plot calibration curve\n",
        "        CalibrationDisplay.from_estimator(\n",
        "            model,\n",
        "            test_df.drop(columns='Class'),\n",
        "            true_labels,\n",
        "            n_bins=10,\n",
        "            name=model_name,\n",
        "            ax=ax_calibration_curve\n",
        "        )\n",
        "        ax_calibration_curve.set_title(f\"Calibration plot ({model_name})\")\n",
        "\n",
        "        # Plot histogram\n",
        "        ax_histogram.hist(prob_pos, range=(0, 1), bins=10, label=model_name)\n",
        "        ax_histogram.set(title=model_name, xlabel=\"Mean predicted probability\", ylabel=\"Count\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    end_time = time.time()  # Record end time\n",
        "    execution_time = end_time - start_time  # Calculate execution time in seconds\n",
        "\n",
        "    # Record execution time in the time DataFrame\n",
        "    resampled_time_df.loc[model_name, 'Execution Time (s)'] = execution_time\n",
        "\n",
        "    return calibrated_performance_df,model"
      ],
      "metadata": {
        "id": "sfIFX8LmJvch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Logistic Regression\n",
        "logistic_regression_model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
        "calibrated_performance_df,_ = evaluate_resampled_calibrated_model_performance(logistic_regression_model,'Logistic Regression', sampler = SMOTE(), verbose=True)"
      ],
      "metadata": {
        "id": "YJsbtAAHKE0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calibrated_performance_df"
      ],
      "metadata": {
        "id": "EDNhnH8vUOnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "techniques = [RandomOverSampler(), SMOTE(), ADASYN(), RandomUnderSampler(), NearMiss(version=1), TomekLinks(), EditedNearestNeighbours()]\n",
        "\n",
        "for sampler in tqdm(techniques):\n",
        "    technique = sampler.__class__.__name__\n",
        "    print(f'Technique: {technique}')\n",
        "    model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)\n",
        "    calibrated_performance_df,_ = evaluate_resampled_calibrated_model_performance(logistic_regression_model,'Logistic Regression', sampler = sampler, verbose=True)"
      ],
      "metadata": {
        "id": "Ampt_lWhVuUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_calibrated_resampling_performance_df = calibrated_performance_df.copy()\n",
        "lr_calibrated_resampling_performance_df.to_csv('lr_calibrated_resampling_performance.csv')"
      ],
      "metadata": {
        "id": "Jzp3qCZLXhH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calibrate CatBoost"
      ],
      "metadata": {
        "id": "prDIsjnbXSn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calibrated_performance_df[:] = np.nan"
      ],
      "metadata": {
        "id": "uEC5lWZFWAA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sampler in tqdm(techniques):\n",
        "    technique = sampler.__class__.__name__\n",
        "\n",
        "    print(f'Technique: {technique}')\n",
        "\n",
        "    catboost_model  = CatBoostClassifier(\n",
        "    task_type=\"CPU\",       # You can change this to \"GPU\" if you have a GPU.\n",
        "    thread_count=-1,       # Use all available CPU cores\n",
        "    verbose=0,\n",
        "    random_state=RANDOM_STATE\n",
        "    )\n",
        "    calibrated_performance_df,_ = evaluate_resampled_calibrated_model_performance(catboost_model,'CatBoost Classifier', sampler = sampler, verbose=True)"
      ],
      "metadata": {
        "id": "4pk_tXDmXzdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "catboost_calibrated_resampling_performance_df = calibrated_performance_df.copy()\n",
        "catboost_calibrated_resampling_performance_df.to_csv('catboost_calibrated_resampling_performance.csv')"
      ],
      "metadata": {
        "id": "hB17ZUqxYAT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pBo45-gLYFjo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}